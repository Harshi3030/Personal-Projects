{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apparel Recommendations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\i331509\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>brand</th>\n",
       "      <th>color</th>\n",
       "      <th>medium_image_url</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>title</th>\n",
       "      <th>formatted_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B004GSI2OS</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>Onyx Black/ Stone</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>featherlite ladies long sleeve stain resistant...</td>\n",
       "      <td>$26.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B012YX2ZPI</td>\n",
       "      <td>HX-Kingdom Fashion T-shirts</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>womens unique 100 cotton  special olympics wor...</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B003BSRPB0</td>\n",
       "      <td>FeatherLite</td>\n",
       "      <td>White</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>featherlite ladies moisture free mesh sport sh...</td>\n",
       "      <td>$20.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>B014ICEJ1Q</td>\n",
       "      <td>FNC7C</td>\n",
       "      <td>Purple</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>supernatural chibis sam dean castiel neck tshi...</td>\n",
       "      <td>$7.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>B01NACPBG2</td>\n",
       "      <td>Fifth Degree</td>\n",
       "      <td>Black</td>\n",
       "      <td>https://images-na.ssl-images-amazon.com/images...</td>\n",
       "      <td>SHIRT</td>\n",
       "      <td>fifth degree womens gold foil graphic tees jun...</td>\n",
       "      <td>$6.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin                        brand              color  \\\n",
       "4   B004GSI2OS                  FeatherLite  Onyx Black/ Stone   \n",
       "6   B012YX2ZPI  HX-Kingdom Fashion T-shirts              White   \n",
       "15  B003BSRPB0                  FeatherLite              White   \n",
       "27  B014ICEJ1Q                        FNC7C             Purple   \n",
       "46  B01NACPBG2                 Fifth Degree              Black   \n",
       "\n",
       "                                     medium_image_url product_type_name  \\\n",
       "4   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "6   https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "15  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "27  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "46  https://images-na.ssl-images-amazon.com/images...             SHIRT   \n",
       "\n",
       "                                                title formatted_price  \n",
       "4   featherlite ladies long sleeve stain resistant...          $26.26  \n",
       "6   womens unique 100 cotton  special olympics wor...           $9.99  \n",
       "15  featherlite ladies moisture free mesh sport sh...          $20.54  \n",
       "27  supernatural chibis sam dean castiel neck tshi...           $7.39  \n",
       "46  fifth degree womens gold foil graphic tees jun...           $6.95  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions which we will use through the rest of the workshop.\n",
    "\n",
    "\n",
    "#Display an image\n",
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it in notebook \n",
    "    plt.imshow(img)\n",
    "\n",
    "def get_word_vec(sentence, doc_id, m_name):\n",
    "    # sentence : title of the apparel\n",
    "    # doc_id: document id in our corpus\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if m_name == 'weighted' and i in  tfidf_title_vectorizer.vocabulary_:\n",
    "                vec.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[i]] * model[i])\n",
    "            elif m_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "        else:\n",
    "            # if the word in our courpus is not there in the google word2vec corpus, we are just ignoring it\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    # we will return a numpy array of shape (#number of words in title * 300 ) 300 = len(w2v_model[word])\n",
    "    # each row represents the word2vec representation of each word (weighted/avg) in given sentance \n",
    "    return  np.array(vec)\n",
    "\n",
    "def get_distance(vec1, vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    \n",
    "    final_dist = []\n",
    "    # for each vector in vec1 we caluclate the distance(euclidean) to all vectors in vec2\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            # np.linalg.norm(i-j) will result the euclidean distance between vectors i, j\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "    # final_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # final_dist[i,j] = euclidean distance between vectors i, j\n",
    "    return np.array(final_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF word to vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map_w2v_brand(sentance1, sentance2, url, doc_id1, doc_id2, df_id1, df_id2, model):\n",
    "    \n",
    "    # sentance1 : title1, input apparel\n",
    "    # sentance2 : title2, recommended apparel\n",
    "    # url: apparel image url\n",
    "    # doc_id1: document id of input apparel\n",
    "    # doc_id2: document id of recommended apparel\n",
    "    # df_id1: index of document1 in the data frame\n",
    "    # df_id2: index of document2 in the data frame\n",
    "    # model: it can have two values, 1. avg 2. weighted\n",
    "    \n",
    "    #s1_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s1_vec = get_word_vec(sentance1, doc_id1, model)\n",
    "    #s2_vec = np.array(#number_of_words_title2 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s2_vec = get_word_vec(sentance2, doc_id2, model)\n",
    "    \n",
    "    # s1_s2_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # s1_s2_dist[i,j] = euclidean distance between words i, j\n",
    "    s1_s2_dist = get_distance(s1_vec, s2_vec)\n",
    "   \n",
    "    data_matrix = [['Asin','Brand', 'Color', 'Product type'],\n",
    "               [data['asin'].loc[df_id1],brands[doc_id1], colors[doc_id1], types[doc_id1]], # input apparel's features\n",
    "               [data['asin'].loc[df_id2],brands[doc_id2], colors[doc_id2], types[doc_id2]]] # recommonded apparel's features\n",
    "    \n",
    "    colorscale = [[0, '#1d004d'],[.5, '#f2e5ff'],[1, '#f2e5d1']] # to color the headings of each column \n",
    "    \n",
    "    # we create a table with the data_matrix\n",
    "    table = ff.create_table(data_matrix, index=True, colorscale=colorscale)\n",
    "    # plot it with plotly\n",
    "    plotly.offline.iplot(table, filename='simple_table')\n",
    "    \n",
    "    # devide whole figure space into 25 * 1:10 grids\n",
    "    gs = gridspec.GridSpec(25, 15)\n",
    "    fig = plt.figure(figsize=(25,5))\n",
    "    \n",
    "    # in first 25*10 grids we plot heatmap\n",
    "    ax1 = plt.subplot(gs[:, :-5])\n",
    "    # ploting the heap map based on the pairwise distances\n",
    "    ax1 = sns.heatmap(np.round(s1_s2_dist,6), annot=True)\n",
    "    # set the x axis labels as recommended apparels title\n",
    "    ax1.set_xticklabels(sentance2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax1.set_yticklabels(sentance1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax1.set_title(sentance2)\n",
    "\n",
    "    # in last 25 * 10:15 grids we display image\n",
    "    ax2 = plt.subplot(gs[:, 10:16])\n",
    "    # we dont display grid lins and axis labels to images\n",
    "    ax2.grid(False)\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    # pass the url it display it\n",
    "    display_img(url, ax2, fig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_model', 'rb') as handle:\n",
    "    model = pickle.load(handle)\n",
    "\n",
    "vocab = model.keys()\n",
    "# this function will add the vectors of each word and returns the avg vector of given sentance\n",
    "def build_avg_vec(sentence, num_features, doc_id, m_name):\n",
    "    # sentace: its title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        nwords += 1\n",
    "        if word in vocab:\n",
    "            if m_name == 'weighted' and word in tfidf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, idf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[word]] * model[word])\n",
    "            elif m_name == 'avg':\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # returns the avg vector of given sentance, its of shape (1, 300)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_w2v_brand(doc_id, text_weight, brand_weight, type_weight, color_weight, image_weight, number_of_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    # w1: weight for  w2v features\n",
    "    # w2: weight for brand and color features\n",
    "\n",
    "    # pairwise_dist will store the distance from given input apparel to all remaining apparels\n",
    "    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "    # http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "    tf_idf_w2v_dist  = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))\n",
    "    brand_feat_dist = pairwise_distances(brand_features, brand_features[doc_id])\n",
    "    type_feat_dist = pairwise_distances(type_features, type_features[doc_id])\n",
    "    color_feat_dist = pairwise_distances(color_features, color_features[doc_id])\n",
    "    pairwise_dist   = (text_weight * tf_idf_w2v_dist + brand_weight * brand_feat_dist + type_weight * type_feat_dist + color_weight * color_feat_dist)/float( text_weight + brand_weight + type_weight + color_weight)\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:number_of_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:number_of_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "\n",
    "    for i in range(0, len(indices)):\n",
    "        heat_map_w2v_brand(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i],df_indices[0], df_indices[i], 'weighted')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('Brand :',data['brand'].loc[df_indices[i]])\n",
    "        print('euclidean distance from input :', pdists[i])\n",
    "        print('='*125)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of the brand values are empty. \n",
    "# Need to replace Null with string \"NULL\"\n",
    "data['brand'].fillna(value=\"Not given\", inplace=True )\n",
    "\n",
    "# replace spaces with hypen\n",
    "brands = [x.replace(\" \", \"-\") for x in data['brand'].values]\n",
    "types = [x.replace(\" \", \"-\") for x in data['product_type_name'].values]\n",
    "colors = [x.replace(\" \", \"-\") for x in data['color'].values]\n",
    "\n",
    "brand_vectorizer = CountVectorizer()\n",
    "brand_features = brand_vectorizer.fit_transform(brands)\n",
    "\n",
    "type_vectorizer = CountVectorizer()\n",
    "type_features = type_vectorizer.fit_transform(types)\n",
    "\n",
    "color_vectorizer = CountVectorizer()\n",
    "color_features = color_vectorizer.fit_transform(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_title_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-b8086f757785>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mw2v_title_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mw2v_title_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_avg_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdoc_id\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-1a5c50554a99>\u001b[0m in \u001b[0;36mbuild_avg_vec\u001b[1;34m(sentence, num_features, doc_id, m_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mnwords\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mm_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'weighted'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtfidf_title_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0mfeatureVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midf_title_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_title_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mm_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'avg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_title_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "doc_id = 272\n",
    "text_weight = 10\n",
    "brand_weight = 5\n",
    "color_weight = 5\n",
    "type_weight = 5\n",
    "image_weight = 5\n",
    "number_of_results = 20\n",
    "w2v_title_weight = []\n",
    "for i in data['title']:\n",
    "    w2v_title_weight.append(build_avg_vec(i, 300, doc_id,'weighted'))\n",
    "    doc_id += 1\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title_weight = np.array(w2v_title_weight)\n",
    "tf_idf_w2v_brand(doc_id, text_weight, brand_weight, type_weight, color_weight, image_weight, number_of_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
